{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd24855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "     -------------------------------------- 316.9/316.9 MB 1.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     -------------------------------------- 200.5/200.5 kB 1.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425388 sha256=9d8f331d7e20ff6fea00619f67b1ad9711f0d86bb5577e258ce779a3f88c4c3b\n",
      "  Stored in directory: c:\\users\\thisa\\appdata\\local\\pip\\cache\\wheels\\72\\3c\\32\\f0f20da5a933f8c6c5a1a2184a9e516652ed3eebeb49f5ddd0\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0040136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccdd42fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('test1part3.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86a866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   name| age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|thisara|  24|        10| 30000|\n",
      "| bishan|  23|         8| 65000|\n",
      "|  udith|  25|         4|100000|\n",
      "| shehan|  27|         2|  5000|\n",
      "|amantha|NULL|      NULL| 75000|\n",
      "|   NULL|  34|        10|200000|\n",
      "|   NULL|  26|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('test1part3.csv',header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc61cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save inside a variable\n",
    "df_pyspark = spark.read.csv('test1part3.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa4b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   name| age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|thisara|  24|        10| 30000|\n",
      "| bishan|  23|         8| 65000|\n",
      "|  udith|  25|         4|100000|\n",
      "| shehan|  27|         2|  5000|\n",
      "|amantha|NULL|      NULL| 75000|\n",
      "|   NULL|  34|        10|200000|\n",
      "|   NULL|  26|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac35413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  24|        10| 30000|\n",
      "|  23|         8| 65000|\n",
      "|  25|         4|100000|\n",
      "|  27|         2|  5000|\n",
      "|NULL|      NULL| 75000|\n",
      "|  34|        10|200000|\n",
      "|  26|      NULL|  NULL|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop the columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf28d351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   name| age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|thisara|  24|        10| 30000|\n",
      "| bishan|  23|         8| 65000|\n",
      "|  udith|  25|         4|100000|\n",
      "| shehan|  27|         2|  5000|\n",
      "|amantha|NULL|      NULL| 75000|\n",
      "|   NULL|  34|        10|200000|\n",
      "|   NULL|  26|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1bbec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|thisara| 24|        10| 30000|\n",
      "| bishan| 23|         8| 65000|\n",
      "|  udith| 25|         4|100000|\n",
      "| shehan| 27|         2|  5000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show() # null value include raws deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5819b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   name| age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|thisara|  24|        10| 30000|\n",
      "| bishan|  23|         8| 65000|\n",
      "|  udith|  25|         4|100000|\n",
      "| shehan|  27|         2|  5000|\n",
      "|amantha|NULL|      NULL| 75000|\n",
      "|   NULL|  34|        10|200000|\n",
      "|   NULL|  26|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## how = all\n",
    "# 'any' drop a row if it contains any nulls\n",
    "# 'all' drop a row only if all its values are null.\n",
    "df_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "307622b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|thisara| 24|        10| 30000|\n",
      "| bishan| 23|         8| 65000|\n",
      "|  udith| 25|         4|100000|\n",
      "| shehan| 27|         2|  5000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c08763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   name| age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|thisara|  24|        10| 30000|\n",
      "| bishan|  23|         8| 65000|\n",
      "|  udith|  25|         4|100000|\n",
      "| shehan|  27|         2|  5000|\n",
      "|amantha|NULL|      NULL| 75000|\n",
      "|   NULL|  34|        10|200000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshold\n",
    "df_pyspark.na.drop(how=\"any\",thresh=2).show()\n",
    "# to not get deleted at least 2 values must there in a row.\n",
    "# but last row is deleted, because there are only one not null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c34bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|thisara| 24|        10| 30000|\n",
      "| bishan| 23|         8| 65000|\n",
      "|  udith| 25|         4|100000|\n",
      "| shehan| 27|         2|  5000|\n",
      "|   NULL| 34|        10|200000|\n",
      "|   NULL| 26|      NULL|  NULL|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## subset\n",
    "df_pyspark.na.drop(how='any',subset=['age']).show()\n",
    "# in 'subset' function we can give a column name, the functionality of 'subset' function is check that column one by one.\n",
    "# if null value meet, then delete entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83aea78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|       thisara|  24|        10| 30000|\n",
      "|        bishan|  23|         8| 65000|\n",
      "|         udith|  25|         4|100000|\n",
      "|        shehan|  27|         2|  5000|\n",
      "|       amantha|NULL|      NULL| 75000|\n",
      "|Missing Values|  34|        10|200000|\n",
      "|Missing Values|  26|      NULL|  NULL|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## filling the missing value\n",
    "df_pyspark.na.fill('Missing Values').show()\n",
    "\n",
    "####################################################################################################################<<--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da189564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|       thisara|  24|        10| 30000|\n",
      "|        bishan|  23|         8| 65000|\n",
      "|         udith|  25|         4|100000|\n",
      "|        shehan|  27|         2|  5000|\n",
      "|       amantha|NULL|      NULL| 75000|\n",
      "|Missing Values|  34|        10|200000|\n",
      "|Missing Values|  26|      NULL|  NULL|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## filling the missing value\n",
    "df_pyspark.na.fill('Missing Values',['name','age']).show()    \n",
    "\n",
    "####################################################################################################################<<--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9c4c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   name| age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|thisara|  24|        10| 30000|\n",
      "| bishan|  23|         8| 65000|\n",
      "|  udith|  25|         4|100000|\n",
      "| shehan|  27|         2|  5000|\n",
      "|amantha|NULL|      NULL| 75000|\n",
      "|   NULL|  34|        10|200000|\n",
      "|   NULL|  26|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faf7d7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|Experience| age|\n",
      "+----------+----+\n",
      "|        10|  24|\n",
      "|         8|  23|\n",
      "|         4|  25|\n",
      "|         2|  27|\n",
      "|      NULL|NULL|\n",
      "|        10|  34|\n",
      "|      NULL|  26|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select specific columns in a dataframe\n",
    "df_pyspark.select('Experience', 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ba777f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values walata mean, median, mode kiyana widi 3n 1k use karala null values fill karanna puluwan apita. \n",
    "# ekata apita use karanna wenawa 'Imputer' function eka\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['age', 'Experience', 'Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['age','Experience','Salary']]\n",
    "    ).setStrategy(\"mean\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcb49060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|thisara|  24|        10| 30000|         24|                10|         30000|\n",
      "| bishan|  23|         8| 65000|         23|                 8|         65000|\n",
      "|  udith|  25|         4|100000|         25|                 4|        100000|\n",
      "| shehan|  27|         2|  5000|         27|                 2|          5000|\n",
      "|amantha|NULL|      NULL| 75000|         26|                 6|         75000|\n",
      "|   NULL|  34|        10|200000|         34|                10|        200000|\n",
      "|   NULL|  26|      NULL|  NULL|         26|                 6|         79166|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66131533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ad80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba44ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d243d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabeab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344b88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b230c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c4d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
